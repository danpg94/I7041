{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librer√≠as del Proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "nltk.download('stopwords')\n",
    " \n",
    "# Importing libraries necessary for Model Building and Training\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    " \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import mailbox\n",
    "from email import policy\n",
    "from email.parser import BytesParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_correo = mailbox.mbox(os.getcwd() + r'\\easy_ham\\0001.ea7e79d3153e7469e7a9c3e0af6a357e')\n",
    "\n",
    "with open(os.getcwd() + r'\\easy_ham\\0001.ea7e79d3153e7469e7a9c3e0af6a357e', 'rb') as fp:\n",
    "    archivo_correo_2 = BytesParser(policy=policy.default).parse(fp)\n",
    "\n",
    "archivo_correo_2.get_payload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for mensaje in archivo_correo:\n",
    "    print(mensaje.is_multipart())\n",
    "    cuerpo = mensaje.get_payload(decode=True).decode(errors='replace')\n",
    "cuerpo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abriendo las carpetas de spam y no spam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_markup(s):\n",
    "    tag = False\n",
    "    quote = False\n",
    "    out = \"\"\n",
    "\n",
    "    for c in s:\n",
    "            if c == '<' and not quote:\n",
    "                tag = True\n",
    "            elif c == '>' and not quote:\n",
    "                tag = False\n",
    "            elif (c == '\"' or c == \"'\") and tag:\n",
    "                quote = not quote\n",
    "            elif not tag:\n",
    "                out = out + c\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANR = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});') \n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "  cleantext = re.sub(CLEANR, '', raw_html)\n",
    "  return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
    "    return punctuationfree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://S+|www.S+')\n",
    "    return url_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    return ''.join(c for c in text if not c.isdigit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stop_words = stopwords.words('english')\n",
    " \n",
    "    imp_words = []\n",
    " \n",
    "    # Storing the important words\n",
    "    for word in str(text).split():\n",
    "        word = word.lower()\n",
    " \n",
    "        if word not in stop_words:\n",
    "            imp_words.append(word)\n",
    " \n",
    "    output = \" \".join(imp_words)\n",
    " \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_parser(mbox, carp):\n",
    "    message = {}\n",
    "    for mensaje in mbox:\n",
    "        cuerpo = \"\"\n",
    "        if mensaje.is_multipart():\n",
    "            # Si el correo tiene varias partes, buscar la parte de texto/html\n",
    "            # message['isMultipart'] = True\n",
    "            for parte in mensaje.get_payload():\n",
    "                if parte.get_content_type() == 'text/plain':\n",
    "                    cuerpo = parte.get_payload(decode=True).decode(errors='replace')\n",
    "                    #cuerpo += cuerpo.translate(str.maketrans('', '', string.punctuation))\n",
    "                if parte.get_content_type() == 'text/html':\n",
    "                    cuerpo = parte.get_payload(decode=True).decode(errors='replace')\n",
    "                    #cuerpo += remove_html_markup(cuerpo)\n",
    "        else:\n",
    "            # message['isMultipart'] = False\n",
    "            # Si el correo es de una sola parte, obtener el cuerpo directamente\n",
    "            if mensaje.get_content_type() == 'text/plain':\n",
    "                char_set = mensaje.get_charset()\n",
    "                cuerpo = mensaje.get_payload(decode=True).decode(errors='replace')\n",
    "                #cuerpo = cuerpo.translate(str.maketrans('', '', string.punctuation))\n",
    "            if mensaje.get_content_type() == 'text/html':\n",
    "                char_set = mensaje.get_charset()\n",
    "                cuerpo = mensaje.get_payload(decode=True).decode(errors='replace')\n",
    "                #cuerpo = remove_html_markup(cuerpo)\n",
    "        message['message'] = cuerpo\n",
    "        if carp == 'easy_ham':\n",
    "            message['type'] = 0\n",
    "        else:\n",
    "            message['type'] = 1\n",
    "\n",
    "        return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carpeta = [\"spam\", \"easy_ham\"]  # Ruta de la carpeta que deseas leer\n",
    "message_list = []\n",
    "for carp in carpeta:\n",
    "    # Obtener la lista de archivos en la carpeta\n",
    "    lista_archivos = [f'{os.path.join(os.getcwd(), carp)}\\\\{x}' for x in os.listdir(os.path.join(os.getcwd(), carp))]\n",
    "    #print(lista_archivos)\n",
    "    # Leer cada archivo de la lista\n",
    "    for archivo in lista_archivos:\n",
    "        if os.path.isfile(archivo):  # Verificar que sea un archivo\n",
    "            # Abrir el dataset de correos\n",
    "            mbox = mailbox.mbox(archivo)\n",
    "            # Recorrer cada correo en el dataset\n",
    "            message = email_parser(mbox, carp)\n",
    "            if message == None:\n",
    "                print(archivo)\n",
    "            else:\n",
    "                \n",
    "                message_list.append(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in message_list:\n",
    "    x[\"message\"] = re.sub(r\"[^\\x00-\\x7F]+\", \"\", str(x[\"message\"]))\n",
    "    x[\"message\"] = x[\"message\"].replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"\\\\u\", \" \").replace(\"\\r\", \" \").lower()\n",
    "    x[\"message\"] = cleanhtml(x[\"message\"])\n",
    "    x[\"message\"] = remove_html_markup(x[\"message\"])\n",
    "    x[\"message\"] = remove_urls(x[\"message\"])\n",
    "    x[\"message\"] = remove_punctuation(x[\"message\"])\n",
    "    x[\"message\"] = remove_numbers(x[\"message\"])\n",
    "    x[\"message\"] = remove_stopwords(x[\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('messages.csv', 'w') as f:\n",
    "    f.write(f'isSpam,message\\n')\n",
    "    for x in message_list:\n",
    "        f.write(f'{x[\"type\"]},{x[\"message\"]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"messages.csv\")\n",
    "data.message = data.message.astype('str')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='isSpam', data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampling to balance the dataset\n",
    "ham_msg = data[data.isSpam == 0]\n",
    "spam_msg = data[data.isSpam == 1]\n",
    "ham_msg = ham_msg.sample(n=len(spam_msg))\n",
    " \n",
    "# Plotting the counts of down sampled dataset\n",
    "balanced_data = ham_msg._append(spam_msg).reset_index(drop=True)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data = balanced_data, x='isSpam')\n",
    "plt.title('Distribution of Ham and Spam email messages after downsampling')\n",
    "plt.xlabel('Message types')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train test split\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(balanced_data['message'],\n",
    "                                                    balanced_data['isSpam'],\n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_word_cloud(data, typ):\n",
    "    email_corpus = \" \".join(data['message'])\n",
    " \n",
    "    plt.figure(figsize=(7, 7))\n",
    " \n",
    "    wc = WordCloud(background_color='black',\n",
    "                   max_words=100,\n",
    "                   width=800,\n",
    "                   height=400,\n",
    "                   collocations=False).generate(email_corpus)\n",
    " \n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.title(f'WordCloud for {typ} emails', fontsize=15)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    " \n",
    "plot_word_cloud(balanced_data[balanced_data['isSpam'] == 0], typ='Non-Spam')\n",
    "plot_word_cloud(balanced_data[balanced_data['isSpam'] == 1], typ='Spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_X)\n",
    "\n",
    "# Convert text to sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_X)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "# Pad sequences to have the same length\n",
    "max_len = 100 # maximum sequence length\n",
    "train_sequences = pad_sequences(train_sequences,\n",
    "\t\t\t\t\t\t\t\tmaxlen=max_len, \n",
    "\t\t\t\t\t\t\t\tpadding='post', \n",
    "\t\t\t\t\t\t\t\ttruncating='post')\n",
    "test_sequences = pad_sequences(test_sequences, \n",
    "\t\t\t\t\t\t\tmaxlen=max_len, \n",
    "\t\t\t\t\t\t\tpadding='post', \n",
    "\t\t\t\t\t\t\ttruncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
    "                                    output_dim=32, \n",
    "                                    input_length=max_len))\n",
    "model.add(tf.keras.layers.LSTM(16))\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    " \n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = True),\n",
    "              metrics = ['accuracy'],\n",
    "              optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(patience=3,\n",
    "                   monitor = 'val_accuracy',\n",
    "                   restore_best_weights = True)\n",
    " \n",
    "lr = ReduceLROnPlateau(patience = 2,\n",
    "                       monitor = 'val_loss',\n",
    "                       factor = 0.5,\n",
    "                       verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_sequences, train_Y,\n",
    "                    validation_data=(test_sequences, test_Y),\n",
    "                    epochs=20, \n",
    "                    batch_size=32,\n",
    "                    callbacks = [lr, es]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_sequences, test_Y)\n",
    "print('Test Loss :',test_loss)\n",
    "print('Test Accuracy :',test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el archivo CSV con los datos preparados\n",
    "data = pd.read_csv(\"messages.csv\")\n",
    "data.message = data.message.astype('str')\n",
    "# Inicializar contadores para spam y no spam\n",
    "spam_count = 0\n",
    "non_spam_count = 0\n",
    "\n",
    "# Funci√≥n para predecir si un correo es spam o no\n",
    "def predict_spam(message_text):\n",
    "    # Convertir el mensaje en secuencias y hacer la predicci√≥n\n",
    "    mensaje_secuencia = tokenizer.texts_to_sequences([message_text])\n",
    "    mensaje_secuencia = pad_sequences(mensaje_secuencia, maxlen=max_len, padding='post', truncating='post')\n",
    "    prediction = model.predict(np.array(mensaje_secuencia))\n",
    "    print(prediction)\n",
    "    return prediction[0][0] >= 0.5  # Probabilidad de corte 0.5\n",
    "\n",
    "# Realizar predicciones y contar la cantidad de correos clasificados como spam y no spam\n",
    "for index, row in data.iterrows():\n",
    "    is_spam = predict_spam(row['message'])\n",
    "    print(f\"Correo: {index} - Es spam: {is_spam}\")\n",
    "\n",
    "    # Incrementar los contadores seg√∫n la predicci√≥n\n",
    "    if is_spam:\n",
    "        spam_count += 1\n",
    "    else:\n",
    "        non_spam_count += 1\n",
    "\n",
    "# Imprimir la cantidad de correos clasificados como spam y no spam\n",
    "print(f\"\\nCantidad de correos clasificados como spam: {spam_count}\")\n",
    "print(f\"Cantidad de correos clasificados como no spam: {non_spam_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
